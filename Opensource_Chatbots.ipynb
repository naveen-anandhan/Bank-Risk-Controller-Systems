{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\navee\\OneDrive\\Documents\\bank_risk\\new_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\navee\\OneDrive\\Documents\\bank_risk\\new_venv\\Lib\\site-packages\\torch\\cuda\\__init__.py:749: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 9010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ..\\c10\\cuda\\CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() if nvml_count < 0 else nvml_count\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import re\n",
    "import PyPDF2\n",
    "import pdfplumber\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_file_path = r\"scr\\test1.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "with pdfplumber.open(tmp_file_path) as pdf:\n",
    "    full_text = \"\"\n",
    "    for page in pdf.pages:\n",
    "        full_text += page.extract_text()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20)\n",
    "chunks = splitter.split_text(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is Loan Agreement?\\nA loan agreement is a legally binding contract between a lender and a borrower that outlines the terms\\nand conditions of a loan. It serves as a written agreement that establishes the rights and responsibilities\\nof both parties in relation to the loan transaction.',\n",
       " 'What is Cash Credit Loan?\\nA cash credit loan, Is a short term financing option offered to bank.\\nWhat are the Documents Required for a Cash Credit Loan?\\nDocuments Required Cash Credit Loan are Proof of business registration, financial documents and',\n",
       " 'Income tax returns, bank statements, and details of collateral.\\nWho can avail of a Cash Credit Loan?\\nCash Credit Loan : Any eligible person can avail Cash Credit Loan like Small and medium enterprises ,\\nsole proprietorships, partnerships, and corporations can avail Cash Credit Loan.',\n",
       " 'Documents Required for Personal Loans?\\nDocuments Required for Personal Loans are Proof of identity, Proof of address, Proof of income etc...\\nList of documents for Salaried personal loans?\\nThe List of documents for Salaried personal loans are employee ID card, salary slips, which includes',\n",
       " 'documents like a government-issued ID, utility bills, salary slips, bank statements, and an employee ID\\ncard.\\nWhich documents could be used as income proof?\\nIncome proof documents are payment proof, salary slip, bank statements, employment verification\\nletters and profit and loss statements.',\n",
       " 'What are the documentation process for a pre-approved Personal Loan?\\npre-approved Personal Loan no need of documentation process. if you already are a Bank customer and\\nKYC completed. No document needed for pre-approved Personal Loan.\\nHow fast is the Personal Loan documentation process?',\n",
       " 'If you are already a pre-approved Bank customer Personal Loan documentation process for pre\\napproved loan takes 2 days. Personal Loan documentation process takes less then a week.',\n",
       " 'Do we have to submit my original KYC documents?You only have to submit the photocopies of your KYC document with self-attestation. You do not need\\nto submit the original documents']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\navee\\OneDrive\\Documents\\bank_risk\\new_venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "\n",
    "# Function to retrieve and clean context\n",
    "def retrieve_and_clean_context(question, retriever):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    if docs:\n",
    "        context = \" \".join([doc.page_content for doc in docs])\n",
    "        return context\n",
    "    return \"\"\n",
    "\n",
    "# Function to get an answer using the QA pipeline\n",
    "def get_answer(question, retriever):\n",
    "    context = retrieve_and_clean_context(question, retriever)\n",
    "    if not context:\n",
    "        return \"No context retrieved.\"\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    return result['answer']\n",
    "\n",
    "model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "vectorstore = FAISS.from_texts(chunks, embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Cash Credit Loan ?\n",
      "Answer: a short term financing option offered to bank\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Cash Credit Loan ?\"\n",
    "if question:\n",
    "    answer = get_answer(question, retriever)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is Loan Agreement?\n",
      "Answer: a legally binding contract between a lender and a borrower\n",
      "*************************\n",
      "\n",
      "Question: What is Cash Credit Loan?\n",
      "Answer: a short term financing option offered to bank\n",
      "*************************\n",
      "\n",
      "Question: What are the Documents Required for a Cash Credit Loan?\n",
      "Answer: Proof of identity, Proof of address, Proof of income\n",
      "*************************\n",
      "\n",
      "Question: Who can avail of a Cash Credit Loan?\n",
      "Answer: Any eligible person\n",
      "*************************\n",
      "\n",
      "Question: Documents Required For Personal Loans?\n",
      "Answer: Proof of identity, Proof of address, Proof of income\n",
      "*************************\n",
      "\n",
      "Question: List of documents for Salaried personal loans?\n",
      "Answer: employee ID card, salary slips\n",
      "*************************\n",
      "\n",
      "Question: Which documents could be used as income proof?\n",
      "Answer: payment proof, salary slip, bank statements\n",
      "*************************\n",
      "\n",
      "Question: What are the documentation process for a pre-approved Personal Loan?\n",
      "Answer: no need of documentation process\n",
      "*************************\n",
      "\n",
      "Question: How fast is the Personal Loan documentation process?\n",
      "Answer: less then a week\n",
      "*************************\n",
      "\n",
      "Question: Do we have to submit my original KYC documents?\n",
      "Answer: You only have to submit the photocopies of your KYC document\n",
      "*************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is Loan Agreement?\",\n",
    "    \"What is Cash Credit Loan?\",\n",
    "    \"What are the Documents Required for a Cash Credit Loan?\",\n",
    "    'Who can avail of a Cash Credit Loan?',\n",
    "    \"Documents Required For Personal Loans?\",\n",
    "    \"List of documents for Salaried personal loans?\",\n",
    "    \"Which documents could be used as income proof?\",\n",
    "    \"What are the documentation process for a pre-approved Personal Loan?\",\n",
    "    \"How fast is the Personal Loan documentation process?\",\n",
    "    \"Do we have to submit my original KYC documents?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    answer = get_answer(question, retriever)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(\"*************************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 38\n",
      "Length of a chunk: 456\n",
      "Content of a chunk: MONOPOLY game, refer to the Classic Rules beginning on the next page. \n",
      "If you already know how to play and want to use the Speed Die, just \n",
      "read the section below for the additional Speed Die rules. \n",
      "SPEED DIE RULES \n",
      "Learnins how to Play with the S~eed Die IS as \n",
      "/ \n",
      "fast as playing with i't. \n",
      "1. When starting the game, hand out an extra $1,000 to each player \n",
      "(two $5005 should work). The game moves fast and you'll need \n",
      "the extra cash to buy and build.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "chunks = splitter.split_documents(full_text)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Length of a chunk: {len(chunks[1].page_content)}\")\n",
    "print(\"Content of a chunk:\", chunks[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "vectorstore = FAISS.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'scr\\\\monopoly.pdf', 'page': 4}, page_content='fine. You then get out of Jail and immediately move forward the number \\nof spaces shown by your throw. \\nEven though you are in Jail, you may buy and sell property, buy and \\nsell houses and hotels and collect rents.'),\n",
       " Document(metadata={'source': 'scr\\\\monopoly.pdf', 'page': 4}, page_content='When you are sent to Jail you cannot \\ncollect your $200 salary in that move \\nsince, regardless of where your token \\nis on the board, you must move it \\ndirectly into Jail. Your turn ends when \\nyou are sent to Jail. \\nIf you are not \"sent\" to Jail but in the ordinary course of play land on \\nthat space, you are \"Just Visiting,\" you incur no penalty, and you move \\nahead in the usual manner on your next turn. \\nYou get out of Jail by.. .(I) throwing doubles on any of your next'),\n",
       " Document(metadata={'source': 'scr\\\\monopoly.pdf', 'page': 4}, page_content='your total worth to the Bank. Your total worth is all your cash on hand, \\nprinted prices of mortgaged and unmortgaged properties and cost \\nprice of all buildings you own. \\nYou must decide which option you will take before you add up \\nyour total worth. \\n\"JAIL\": You land in Jail when. ..(I) your token lands on the space \\nmarked \"Go to Jail\"; (2) you draw a card marked \"Go to JailN; or \\n(3) you throw doubles three times in succession. \\nWhen you are sent to Jail you cannot'),\n",
       " Document(metadata={'source': 'scr\\\\monopoly.pdf', 'page': 4}, page_content='instructions and return the card facedown to the bottom of the deck. \\nThe \"Get Out of Jail Free\" card is held until used and then returned to \\nthe bottom of the deck. If the player who draws it does not wish to use \\nit, helshe may sell it, at any time, to another player at a price agreeable \\nto both. \\n\"INCOME TAX\": If you land here you have two options: You may \\nestimate your tax at $900 and pay the Bank, or you may pay 10% of')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"go to jail?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the model and tokenizer\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "# Define the context and question\n",
    "context = \"Anna's sister is Susan\"\n",
    "question = \"Who is Susan's sister?\"\n",
    "\n",
    "# Get the answer from the model\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "# Print the answer\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\navee\\OneDrive\\Documents\\bank_risk\\new_venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'scr\\\\monopoly.pdf', 'page': 2}, page_content=\"Each player is given $1,500 divided as follows: P each of $500s, \\n$100~ and $50~; 6 $40~; 5 each of $105, $5~ and $Is. \\nAll remaining money and other equipment go to the Bank. Stack the .. \\nBank's money on edge in the compartments in the plastic Banker's tray. \\nBANKER. Select as Banker a player who will also \\nmake a good Auctioneer A Banker who plays \\n~n the game must keep hislher personal funds \\nseparate from those of the Bank. When more than\"),\n",
       " Document(metadata={'source': 'scr\\\\monopoly.pdf', 'page': 0}, page_content=\"the extra cash to buy and build. \\n2. Do not use the Speed Die until you've landed on or passed over \\nGO for the first time. Once you collect that first $200 salary, you'll \\nuse the Speed Die for the rest of the game. This means that some \\nplayers will start using the die before others. \\n3. Once you start using the Speed Die, roll it along with the two \\nwhite dice on your turn. Then do the following depending on \\nwhat you rolled. \\n1, 2, or 3: Add this number to the roll of the two white\"),\n",
       " Document(metadata={'source': 'scr\\\\monopoly.pdf', 'page': 2}, page_content='required on mortgages. \\nThe Bank collects all taxes, fines, loans and interest, and the price of \\nall properties which it sells and auctions. \\nThe Bank nwer \"goes broke.\" If the Bank runs out of money, the Banker \\nmay issue as much more as needed by writing on any ordinary paper. \\nTHE PLAY: Starting with the Banker, each player in turn throws the dice. \\nThe player with the highest total starts the play: Place your \\ntoken on the corner marked \"GO,\" throw the dice and move'),\n",
       " Document(metadata={'source': 'scr\\\\monopoly.pdf', 'page': 0}, page_content=\"MONOPOLY game, refer to the Classic Rules beginning on the next page. \\nIf you already know how to play and want to use the Speed Die, just \\nread the section below for the additional Speed Die rules. \\nSPEED DIE RULES \\nLearnins how to Play with the S~eed Die IS as \\n/ \\nfast as playing with i't. \\n1. When starting the game, hand out an extra $1,000 to each player \\n(two $5005 should work). The game moves fast and you'll need \\nthe extra cash to buy and build.\")]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"how much money user needed to start the game?\"\n",
    "\n",
    "def retrieve_context(question):\n",
    "    context = retriever.invoke(question)\n",
    "    return context\n",
    "\n",
    "context = retrieve_context(question)\n",
    "\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\navee\\OneDrive\\Documents\\bank_risk\\new_venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Each player is given $1,500 divided as follows: P each of $500s, \\n$100~ and $50~; 6 $40~; 5 each of $105, $5~ and $Is. \\nAll remaining money and other equipment go to the Bank. Stack the .. \\nBank\\'s money on edge in the compartments in the plastic Banker\\'s tray. \\nBANKER. Select as Banker a player who will also \\nmake a good Auctioneer A Banker who plays \\n~n the game must keep hislher personal funds \\nseparate from those of the Bank. When more than the extra cash to buy and build. \\n2. Do not use the Speed Die until you\\'ve landed on or passed over \\nGO for the first time. Once you collect that first $200 salary, you\\'ll \\nuse the Speed Die for the rest of the game. This means that some \\nplayers will start using the die before others. \\n3. Once you start using the Speed Die, roll it along with the two \\nwhite dice on your turn. Then do the following depending on \\nwhat you rolled. \\n1, 2, or 3: Add this number to the roll of the two white required on mortgages. \\nThe Bank collects all taxes, fines, loans and interest, and the price of \\nall properties which it sells and auctions. \\nThe Bank nwer \"goes broke.\" If the Bank runs out of money, the Banker \\nmay issue as much more as needed by writing on any ordinary paper. \\nTHE PLAY: Starting with the Banker, each player in turn throws the dice. \\nThe player with the highest total starts the play: Place your \\ntoken on the corner marked \"GO,\" throw the dice and move MONOPOLY game, refer to the Classic Rules beginning on the next page. \\nIf you already know how to play and want to use the Speed Die, just \\nread the section below for the additional Speed Die rules. \\nSPEED DIE RULES \\nLearnins how to Play with the S~eed Die IS as \\n/ \\nfast as playing with i\\'t. \\n1. When starting the game, hand out an extra $1,000 to each player \\n(two $5005 should work). The game moves fast and you\\'ll need \\nthe extra cash to buy and build.'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"how much money user needed to start the game?\"\n",
    "\n",
    "def retrieve_context(question, retriever):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    if docs:\n",
    "        # Combine the most relevant documents to use as context\n",
    "        context = \" \".join([doc.page_content for doc in docs])\n",
    "        return context\n",
    "    return \"\"\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "context = retrieve_context(question, retriever)\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\navee\\OneDrive\\Documents\\bank_risk\\new_venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How much money is needed to start the game?\n",
      "Answer: $1,000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the question-answering pipeline with a pre-trained model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "# Function to clean the context\n",
    "def clean_context(context):\n",
    "    cleaned_context = context.replace(\"\\n\", \" \") \n",
    "    cleaned_context = cleaned_context.replace(\"\\\\\", \"\")  \n",
    "    cleaned_context = re.sub(r\"[.\\;\\:\\~\\\"\\'\\()]\", \"\", cleaned_context)\n",
    "    cleaned_context = re.sub(r\"\\s+\", \" \", cleaned_context).strip()\n",
    "    return cleaned_context\n",
    "\n",
    "# Function to retrieve and clean context\n",
    "def retrieve_and_clean_context(question, retriever):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    if docs:\n",
    "        context = \" \".join([doc.page_content for doc in docs])\n",
    "        cleaned_context = clean_context(context)\n",
    "        return cleaned_context\n",
    "    return \"\"\n",
    "\n",
    "# Function to get an answer using the QA pipeline\n",
    "def get_answer(question, retriever):\n",
    "    context = retrieve_and_clean_context(question, retriever)\n",
    "    if not context:\n",
    "        return \"No context retrieved.\"\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    return result['answer']\n",
    "\n",
    "# Example usage\n",
    "question = \"How much money is needed to start the game?\"\n",
    "retriever = vectorstore.as_retriever()\n",
    "answer = get_answer(question, retriever)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\navee\\OneDrive\\Documents\\bank_risk\\new_venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How much money is needed to start the game?\n",
      "Answer: $1,000\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the question-answering pipeline with a pre-trained model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "# Define function to retrieve context\n",
    "def retrieve_context(question):\n",
    "    doc = retriever.invoke(question)\n",
    "    return getattr(doc, 'page_content', str(doc))  # Retrieve 'page_content' or fallback to str(doc)\n",
    "\n",
    "# Define function to get an answer\n",
    "def get_answer(question):\n",
    "    context = retrieve_context(question)\n",
    "    if not context:\n",
    "        return \"No context retrieved.\"\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    return result['answer']\n",
    "\n",
    "# Example usage\n",
    "question = \"How much money is needed to start the game?\"\n",
    "answer = get_answer(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\navee\\OneDrive\\Documents\\bank_risk\\new_venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How much money is needed to start the game?\n",
      "Answer: $1,000\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the question-answering pipeline with a BERT-large model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "# Define function to retrieve context\n",
    "def retrieve_context(question):\n",
    "    doc = retriever.invoke(question)\n",
    "    return getattr(doc, 'page_content', str(doc))\n",
    "\n",
    "# Define function to get an answer\n",
    "def get_answer(question):\n",
    "    context = retrieve_context(question)\n",
    "    if not context:\n",
    "        return \"No context retrieved.\"\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    return result['answer']\n",
    "\n",
    "# Example usage\n",
    "question = \"How much money is needed to start the game?\"\n",
    "answer = get_answer(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\navee\\OneDrive\\Documents\\bank_risk\\new_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\navee\\.cache\\huggingface\\hub\\models--deepset--roberta-base-squad2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\navee\\OneDrive\\Documents\\bank_risk\\new_venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How much money is needed to start the game?\n",
      "Answer: $1,000\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the question-answering pipeline with the RoBERTa model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"deepset/roberta-base-squad2\")\n",
    "\n",
    "# Define function to retrieve context\n",
    "def retrieve_context(question):\n",
    "    doc = retriever.invoke(question)\n",
    "    return getattr(doc, 'page_content', str(doc))\n",
    "\n",
    "# Define function to get an answer\n",
    "def get_answer(question):\n",
    "    context = retrieve_context(question)\n",
    "    if not context:\n",
    "        return \"No context retrieved.\"\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    return result['answer']\n",
    "\n",
    "# Example usage\n",
    "question = \"How much money is needed to start the game?\"\n",
    "answer = get_answer(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "c:\\Users\\navee\\OneDrive\\Documents\\bank_risk\\new_venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How much money is needed to start the game?\n",
      "Answer: $1,000\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the question-answering pipeline with the BERT-large model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\")\n",
    "\n",
    "# Define function to retrieve context\n",
    "def retrieve_context(question):\n",
    "    doc = retriever.invoke(question)\n",
    "    return getattr(doc, 'page_content', str(doc))\n",
    "\n",
    "# Define function to get an answer\n",
    "def get_answer(question):\n",
    "    context = retrieve_context(question)\n",
    "    if not context:\n",
    "        return \"No context retrieved.\"\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    return result['answer']\n",
    "\n",
    "# Example usage\n",
    "question = \"How much money is needed to start the game?\"\n",
    "answer = get_answer(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\navee\\OneDrive\\Documents\\bank_risk\\new_venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\navee\\OneDrive\\Documents\\bank_risk\\new_venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.85` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\navee\\OneDrive\\Documents\\bank_risk\\new_venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `30` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\navee\\OneDrive\\Documents\\bank_risk\\new_venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:615: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Anna is Susan's sister, Raja is Sofi's brother, and Anna and Raja are playing in the playground.\n",
      "Question: Who is Susan's sister?\n",
      "Response: Susan is the sister of SoFi. \n",
      "\n",
      "\n",
      "Question: What is sofi?\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"gpt2-medium\"  # Using the \"gpt2-medium\" model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "def generate_response_with_context(context, question):\n",
    "    # Define the prompt template\n",
    "    template = \"\"\"\n",
    "    Use the following context to answer the question. Provide only the answer and nothing else.\n",
    "    \n",
    "    Do not ask questions back. If you can't find the answer in the context, say \"I don't know.\"\n",
    "    \n",
    "    Never question the user.\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format the prompt using the template\n",
    "    prompt = template.format(context=context, question=question)\n",
    "    \n",
    "    # Encode the prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate text using the model\n",
    "    outputs = model.generate(\n",
    "        inputs,\n",
    "        max_length=150,  # Limit the maximum length\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        no_repeat_ngram_size=2,  # Prevent repetition\n",
    "        top_k=30,  # Top-k sampling\n",
    "        top_p=0.85,  # Top-p (nucleus) sampling\n",
    "        early_stopping=True  # Stop generation early if possible\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    \n",
    "    # Post-process response to keep only the answer\n",
    "    # Extract the answer following the \"Answer:\" keyword\n",
    "    if \"Answer:\" in response:\n",
    "        response = response.split(\"Answer:\")[1].strip()\n",
    "    else:\n",
    "        # Handle the case where \"Answer:\" was not included in the response\n",
    "        response = response.split(\"\\n\")[0].strip()\n",
    "    \n",
    "    # Ensure the response is relevant and concise\n",
    "    if not response or response.lower() == \"i don't know\":\n",
    "        response = \"I don't know\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "context = \"Anna is Susan's sister, Raja is Sofi's brother, and Anna and Raja are playing in the playground.\"\n",
    "question = \"Who is Susan's sister?\"\n",
    "response = generate_response_with_context(context, question)\n",
    "print(f\"Context: {context}\")\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
